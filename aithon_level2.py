# -*- coding: utf-8 -*-
"""aithon_level2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vBrF0e6TWQUZh6PR7EqDjPx0TjqAr_P6
"""

import numpy as np
import pandas as pd
import os
import cv2
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
import tensorflow as tf
from tensorflow.keras.layers import Dense, Dropout, Flatten,Activation, MaxPooling2D, Input, AveragePooling2D, Conv2D, BatchNormalization
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.models import load_model


def img_transform(img):
  '''
  Histogram Equalization for Image Enhancement
  '''
  image = cv2.equalizeHist(img)
  return image

def Sequential_Model():
  '''
  Custom Convolutional Neural Network
  '''

  # Initialising the CNN
  model = Sequential([
  # 1 - Convolution
  Conv2D(64,(3,3), padding='same', input_shape=(48, 48,1)),
  BatchNormalization(),
  Activation('relu'),
  MaxPooling2D(pool_size=(2, 2)),
  Dropout(0.25),

  # 2nd Convolution layer
  Conv2D(128,(5,5), padding='same'),
  BatchNormalization(),
  Activation('relu'),
  MaxPooling2D(pool_size=(2, 2)),
  Dropout(0.25),

  # 3rd Convolution layer
  Conv2D(512,(3,3), padding='same'),
  BatchNormalization(),
  Activation('relu'),
  MaxPooling2D(pool_size=(2, 2)),
  Dropout(0.25),

  # 4th Convolution layer
  Conv2D(512,(3,3), padding='same'),
  BatchNormalization(),
  Activation('relu'),
  MaxPooling2D(pool_size=(2, 2)),
  Dropout(0.25),

  # Flattening
  Flatten(),

  # Fully connected layer 1st layer
  Dense(256),
  BatchNormalization(),
  Activation('relu'),
  Dropout(0.25),

  # Fully connected layer 2nd layer
  Dense(512),
  BatchNormalization(),
  Activation('relu'),
  Dropout(0.25),

  Dense(3, activation='softmax')
  ])
  return model

def test_fun(model,test_file):
  data_test = pd.read_csv(test_file)
  df_test = pd.DataFrame(data_test)
  num_of_instances = df_test.shape[0]
  data = []

  #Pixel to Numpy#
  #Labels enocoding#
  for i in range(num_of_instances):
    if True: #**Drop here#
      img = df_test.iloc[i, 1:].values.tolist()
      pixels = np.array(img, 'uint8')
      emotion = df_test.iloc[i, 0].lower()
      image = pixels.reshape(48,48)
      data.append(image)
    else:
      pass
  
  X = np.array(data)
  #Calling Histogram equalization#
  for i in range(X.shape[0]):
    X[i] = img_transform(X[i])

  #Normalization#
  X = X.reshape(X.shape[0], 48, 48, 1).astype('float32')
  X = X/255.0

  #Loading the model#
  model.load_weights("cyberpunk_model.hdf5")

  prediction_arr = np.array(model.predict(X))

  _class = []
  for arr in prediction_arr:
    if np.argmax(arr) == 0:
      _class.append("fear")
    if np.argmax(arr) == 1:
      _class.append("sad")
    if np.argmax(arr) == 2:
      _class.append("happy")

  return _class



def aithon_level2_api(train_file,test_file):
  data_train = pd.read_csv(train_file)
  df_train = pd.DataFrame(data_train)

  num_of_instances = df_train.shape[0]

  data = []
  labels = []
  #Pixel to Numpy#
  #Labels enocoding#
  for i in range(num_of_instances):
    if True: #****Drop here#
      img = df_train.iloc[i, 1:].values.tolist()
      pixels = np.array(img, 'uint8')
      emotion = df_train.iloc[i, 0].lower()
      image = pixels.reshape(48,48)

      data.append(image)

      if(emotion == 'fear' or emotion=='fearness'):
        labels.append(0)
      if(emotion == 'sad' or emotion=='sadness'):
        labels.append(1)
      if(emotion == 'happy' or emotion=='happyness'):
        labels.append(2)
    else:
      pass
  
  X = np.array(data)
  Y = np.array(labels)

  #Calling Histogram equalization#
  for i in range(X.shape[0]):
    X[i] = img_transform(X[i])
  stratSplit = StratifiedShuffleSplit(n_splits = 5, test_size=0.25, random_state=42)

  for train_idx, valid_idx in stratSplit.split(X,Y):
    X_train, X_valid = X[train_idx], X[valid_idx]
    Y_train, Y_valid= Y[train_idx], Y[valid_idx]

  X_train = X_train.reshape(X_train.shape[0], 48, 48, 1).astype('float32')
  X_valid = X_valid.reshape(X_valid.shape[0], 48, 48, 1).astype('float32')

  X_train /= 255
  X_valid /= 255

  n_classes = 3

  Y_train = tf.keras.utils.to_categorical(Y_train, n_classes)
  Y_valid = tf.keras.utils.to_categorical(Y_valid, n_classes)

  #Face Emotion Detection Model#
  model = Sequential_Model()

  opt = tf.keras.optimizers.Nadam()
  model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['categorical_accuracy', 'acc'])

  #Model Training#
  reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,
                              patience=2, min_lr=0.00001, mode='auto')
  earlystopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)
  checkpoint = ModelCheckpoint('cyberpunk_model.hdf5', monitor='val_sparse_categorical_accuracy',
                             save_best_only=True, mode='max', verbose=1)
  callbacks = [checkpoint, reduce_lr, earlystopping]
  history = model.fit(X_train, Y_train, batch_size=28, epochs=100, verbose=1, callbacks=[callbacks, checkpoint], validation_data=(X_valid, Y_valid))

  #Testing#
  return test_fun(model,test_file)

if __name__ == "__main___":
  input_file_path_train = input("Enter training file\n")
  input_file_path_test = input("Enter testing file\n")
  out = aithon_level2_api(input_file_path_train,input_file_path_test)
  print("\n\n\n")
  print(out)

